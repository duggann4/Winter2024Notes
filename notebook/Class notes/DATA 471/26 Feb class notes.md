Student lecture day 1

# Adam optimizer
optimizers:
- argmin
- Types:
	- SGD
	- momentum; based on previous steps - AR-like model?
	- adagrad
	- RMS prop
ADAM: 2014 combo of momentum and RMS
- closed form looks kind like a timeseries thing?
- Drinking game: take a shot every time Dylan says "um"
- lagging effect?
- first moment estimate: moving average of the gradient
- second moment: does something
- more than one hyperparameter - pain to fit
- high potential for overfit
- training a model the hardest part - descends loss at a faster rate
- ADAM-W completely different
- 
- 