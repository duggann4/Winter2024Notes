- Backprop: exponentially harder to calculate gradients, efficiency in not having to re-compute parameters by working backwards: dynamic programming version of computing gradients
- NN: composition of a bunch of differentiable functions
- double-decent exists occasionally
- deep vs. single-layer:
	- suburbs vs skyscraper
	- fewer parameters; less prone to overfit
	- composing building blocks vs describing building blocks
TensorFlow Playground is a thing that exists
Can't learn xor in a single layer - ish