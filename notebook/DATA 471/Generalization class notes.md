Linear regression videos for tomorrow
paper 1 posting tonight

Models learning the model family
- high risk of overfit
- overhead
- Letting the validation set be the guide

In general, training is a fixed and separate process
caveats:
- online learning: continue to train at evaluation (ARIMA)
- transductive learning: test set **inputs** are known during training
- inductive learning: able to handle arbitrary inputs

What are the attributes of ChatGPT?
- probibalistic
- generative
- inductive
- parametric
	- Can throw away training data
	- non-parametric: K-nearest neighbors classifiers

Loss, risk, emperical risk
- loss: error of a single datapoint
- emperical risk: average loss on training set
- risk: expected loss in the universe, unobservable

Bias: How far off the average model is from the true model

Octave

- 
- 